{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Sources\n","- Regressor idea: https://www.kaggle.com/code/tsunotsuno/updated-debertav3-lgbm-with-feature-engineering\n","### Previous notebook:\n","- https://www.kaggle.com/code/josemariasabater/commonlit-roberta-base-with-prompts/edit/run/139550119"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-08-22T10:59:34.871975Z","iopub.status.busy":"2023-08-22T10:59:34.871501Z","iopub.status.idle":"2023-08-22T10:59:50.383176Z","shell.execute_reply":"2023-08-22T10:59:50.381809Z","shell.execute_reply.started":"2023-08-22T10:59:34.871938Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing d:\\kaggle\\input\\pyspellchecker\\pyspellchecker-0.7.2-py3-none-any.whl\n"]},{"name":"stderr","output_type":"stream","text":["WARNING: Requirement '/kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl' looks like a filename, but the file does not exist\n","ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'D:\\\\kaggle\\\\input\\\\pyspellchecker\\\\pyspellchecker-0.7.2-py3-none-any.whl'\n","\n"]}],"source":["!pip3 install \"/kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\""]},{"cell_type":"markdown","metadata":{},"source":["## Imports and Settings"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-08-22T11:00:29.241226Z","iopub.status.busy":"2023-08-22T11:00:29.240949Z","iopub.status.idle":"2023-08-22T11:00:29.252522Z","shell.execute_reply":"2023-08-22T11:00:29.251270Z","shell.execute_reply.started":"2023-08-22T11:00:29.241198Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import warnings\n","import os\n","import shutil\n","import logging\n","import json\n","import transformers\n","from transformers import (\n","    AutoModel,\n","    AutoTokenizer,\n","    AutoConfig,\n","    AutoModelForSequenceClassification,\n",")\n","from transformers import DataCollatorWithPadding\n","from datasets import Dataset, load_dataset, load_from_disk\n","from transformers import TrainingArguments, Trainer\n","from datasets import load_metric, disable_progress_bar\n","from sklearn.metrics import mean_squared_error\n","import torch\n","\n","from sklearn.model_selection import KFold, GroupKFold\n","from tqdm import tqdm\n","import nltk\n","from nltk.corpus import stopwords\n","from collections import Counter\n","import spacy\n","import pandas as pd\n","from spellchecker import SpellChecker\n","import re\n","import string\n","\n","%load_ext lab_black\n","\n","# logging settings\n","\n","# warnings.simplefilter(\"ignore\")\n","logging.disable(logging.ERROR)\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n","# disable_progress_bar()\n","tqdm.pandas()"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-08-22T11:00:43.423738Z","iopub.status.busy":"2023-08-22T11:00:43.423398Z","iopub.status.idle":"2023-08-22T11:00:43.438769Z","shell.execute_reply":"2023-08-22T11:00:43.437485Z","shell.execute_reply.started":"2023-08-22T11:00:43.423696Z"},"trusted":true},"outputs":[],"source":["# set random seed\n","def seed_everything(seed: int):\n","    import random, os\n","    import numpy as np\n","    import torch\n","\n","    random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","\n","\n","seed_everything(seed=42)"]},{"cell_type":"markdown","metadata":{},"source":["### Config class"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-08-22T11:01:11.398783Z","iopub.status.busy":"2023-08-22T11:01:11.398492Z","iopub.status.idle":"2023-08-22T11:01:11.406021Z","shell.execute_reply":"2023-08-22T11:01:11.404775Z","shell.execute_reply.started":"2023-08-22T11:01:11.398754Z"},"trusted":true},"outputs":[],"source":["class CFG:\n","    model_name = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n","    # model_name = \"/kaggle/input/debertav3base\"\n","    learning_rate = 1.2e-5\n","    weight_decay = 0.02\n","    hidden_dropout_prob = 0.1\n","    attention_probs_dropout_prob = 0.01\n","    num_train_epochs = 3\n","    n_splits = 4\n","    batch_size = 8\n","    random_seed = 42\n","    save_steps = 100\n","    max_length = 512\n","    use_prompts = False\n","    warmup_ratio = 0.01"]},{"cell_type":"markdown","metadata":{},"source":["## Load Data"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-08-22T11:03:02.136634Z","iopub.status.busy":"2023-08-22T11:03:02.136295Z","iopub.status.idle":"2023-08-22T11:03:02.320916Z","shell.execute_reply":"2023-08-22T11:03:02.319989Z","shell.execute_reply.started":"2023-08-22T11:03:02.136602Z"},"trusted":true},"outputs":[],"source":["# DATA_DIR = \"/kaggle/input/commonlit-evaluate-student-summaries/\"\n","\n","# prompts_train = pd.read_csv(DATA_DIR + \"prompts_train.csv\")\n","# prompts_test = pd.read_csv(DATA_DIR + \"prompts_test.csv\")\n","# summaries_train = pd.read_csv(DATA_DIR + \"summaries_train.csv\")\n","# summaries_test = pd.read_csv(DATA_DIR + \"summaries_test.csv\")\n","# sample_submission = pd.read_csv(DATA_DIR + \"sample_submission.csv\")\n","\n","# Local\n","\n","DATA_DIR = \"./data/\"\n","\n","prompts_train = pd.read_csv(DATA_DIR + \"prompts_train.csv\")\n","prompts_test = pd.read_csv(DATA_DIR + \"prompts_test.csv\")\n","summaries_train = pd.read_csv(DATA_DIR + \"summaries_train.csv\")\n","summaries_test = pd.read_csv(DATA_DIR + \"summaries_test.csv\")\n","sample_submission = pd.read_csv(DATA_DIR + \"sample_submission.csv\")"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"text/plain":["student_id                                              005ab0199905\n","prompt_id                                                     3b9047\n","text               The highest class was Pharaohs these people we...\n","content                                                    -0.210614\n","wording                                                    -0.471415\n","prompt_question    In complete sentences, summarize the structure...\n","prompt_title                               Egyptian Social Structure\n","prompt_text        Egyptian society was structured like a pyramid...\n","Name: 3, dtype: object"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["# For testing\n","random_nr = np.random.randint(0, len(prompts_train))\n","merged_train = pd.merge(summaries_train, prompts_train, how=\"left\", on=\"prompt_id\")\n","example1 = merged_train.iloc[random_nr].copy()\n","example1"]},{"cell_type":"markdown","metadata":{},"source":["## Preprocessing Class"]},{"cell_type":"markdown","metadata":{},"source":["### Ideas\n","Overlaps\n","Quotes\n","Length of Summary vs Length of text\n","Grammar mistakes\n","Repeated vocabulary inside the summary"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["class Preprocessor:\n","    def __init__(self, model_name: str) -> None:\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        # Spacy NER count\n","        self.spacy_ner_model = spacy.load(\n","            \"en_core_web_sm\",\n","        )\n","        self.speller = SpellChecker()\n","        self.STOP_WORDS = set(stopwords.words(\"english\"))\n","\n","    def count_text_length(self, df: pd.DataFrame, column: str) -> pd.Series:\n","        return df[column].progress_apply(lambda x: len(self.tokenizer.encode(x)))\n","\n","    def non_stop_word_overlap(self, row: pd.Series) -> float:\n","        \"\"\"intersection(prompt_text, text) after removing stop words\"\"\"\n","\n","        def check_is_stop_word(word):\n","            normalized_word = word.lower().strip(\"▁\")\n","            return (\n","                normalized_word not in self.STOP_WORDS\n","                and normalized_word not in string.punctuation\n","            )\n","\n","        prompt_words = row[\"prompt_tokens\"]\n","        summary_words = row[\"summary_tokens\"]\n","        # Remove stop words\n","        prompt_words = list(filter(check_is_stop_word, prompt_words))\n","        summary_words = list(filter(check_is_stop_word, summary_words))\n","\n","        return len(set(prompt_words).intersection(set(summary_words)))\n","\n","    def ngrams(self, input_list: list, n: int) -> list[str]:\n","        \"\"\"Returns a list of ngrams\"\"\"\n","        ngrams_ = zip(*[input_list[i:] for i in range(n)])\n","        return [\" \".join(ngram) for ngram in ngrams_]\n","\n","    def get_ngram_overlap(self, row: pd.Series, n: int) -> float:\n","        \"\"\"Returns the ngram overlap between prompt and summary\"\"\"\n","        summary_ngrams = self.ngrams(row[\"summary_tokens\"], n)\n","        prompt_ngrams = self.ngrams(row[\"prompt_tokens\"], n)\n","\n","        return len(set(summary_ngrams).intersection(set(prompt_ngrams)))\n","\n","    def get_ner_overlap(self, row: pd.Series) -> float:\n","        \"\"\"Returns the number of overlapping named entities between prompt and summary\"\"\"\n","        prompt_doc = self.spacy_ner_model(row[\"prompt_text\"])\n","        summary_doc = self.spacy_ner_model(row[\"text\"])\n","\n","        prompt_entities = set([ent.text.lower() for ent in prompt_doc.ents])\n","        summary_entities = set([ent.text.lower() for ent in summary_doc.ents])\n","\n","        return len(prompt_entities.intersection(summary_entities))\n","\n","    def get_spelling_error_count(self, row: pd.Series) -> float:\n","        \"\"\"Returns the number of spelling errors in the summary\"\"\"\n","        summary_text = row[\"text\"]\n","        text = \"\".join(char for char in summary_text if char not in string.punctuation)\n","        misspelled = self.speller.unknown(text.split())\n","        return len(misspelled)\n","\n","    def run(self, prompts: pd.DataFrame, summaries: pd.DataFrame) -> pd.DataFrame:\n","        # Tokenize\n","\n","        tqdm.pandas(desc=\"Tokenizing Prompts\")\n","        prompts[\"prompt_tokens\"] = prompts[\"prompt_text\"].progress_apply(\n","            lambda x: self.tokenizer.convert_ids_to_tokens(\n","                self.tokenizer.encode(x), skip_special_tokens=True\n","            )\n","        )\n","        tqdm.pandas(desc=\"Tokenizing Summaries\")\n","        summaries[\"summary_tokens\"] = summaries[\"text\"].progress_apply(\n","            lambda x: self.tokenizer.convert_ids_to_tokens(\n","                self.tokenizer.encode(x), skip_special_tokens=True\n","            )\n","        )\n","\n","        merged_df = pd.merge(summaries, prompts, how=\"left\", on=\"prompt_id\")\n","\n","        # Count text length\n","\n","        merged_df[\"prompt_length\"] = self.count_text_length(merged_df, \"prompt_text\")\n","        merged_df[\"summary_length\"] = self.count_text_length(merged_df, \"text\")\n","\n","        # Count non-stop word overlap\n","        tqdm.pandas(desc=\"Counting non-stop word overlap\")\n","        merged_df[\"non_stop_word_overlap\"] = merged_df.progress_apply(\n","            self.non_stop_word_overlap, axis=1\n","        )\n","\n","        # Count ngram overlap\n","        tqdm.pandas(desc=\"Counting unigram overlap\")\n","        merged_df[\"unigram_overlap\"] = merged_df.progress_apply(\n","            lambda x: self.get_ngram_overlap(x, 1), axis=1\n","        )\n","        tqdm.pandas(desc=\"Counting bigram overlap\")\n","        merged_df[\"bigram_overlap\"] = merged_df.progress_apply(\n","            lambda x: self.get_ngram_overlap(x, 2), axis=1\n","        )\n","        tqdm.pandas(desc=\"Counting trigram overlap\")\n","        merged_df[\"trigram_overlap\"] = merged_df.progress_apply(\n","            lambda x: self.get_ngram_overlap(x, 3), axis=1\n","        )\n","\n","        # Count named entity overlap\n","        tqdm.pandas(desc=\"Counting named entity overlap\")\n","        merged_df[\"ner_overlap\"] = merged_df.progress_apply(\n","            self.get_ner_overlap, axis=1\n","        )\n","\n","        # Count spelling errors\n","        tqdm.pandas(desc=\"Counting spelling errors\")\n","        merged_df[\"spelling_error_count\"] = merged_df.progress_apply(\n","            self.get_spelling_error_count, axis=1\n","        )\n","\n","        # Summary/Prompt token length ratio\n","        merged_df[\"token_length_ratio\"] = (\n","            merged_df[\"summary_length\"] / merged_df[\"prompt_length\"]\n","        )\n","        return merged_df\n","\n","\n","Preprocessor = Preprocessor(CFG.model_name)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Tokenizing Prompts: 100%|██████████| 4/4 [00:00<00:00, 54.77it/s]\n","Tokenizing Summaries: 100%|██████████| 7165/7165 [00:11<00:00, 601.01it/s]\n","Tokenizing Summaries: 100%|██████████| 7165/7165 [00:13<00:00, 544.57it/s]\n","Tokenizing Summaries: 100%|██████████| 7165/7165 [00:01<00:00, 4032.07it/s]\n","Counting non-stop word overlap: 100%|██████████| 7165/7165 [00:02<00:00, 3543.53it/s]\n","Counting unigram overlap: 100%|██████████| 7165/7165 [00:00<00:00, 11391.11it/s]\n","Counting bigram overlap: 100%|██████████| 7165/7165 [00:01<00:00, 5567.21it/s]\n","Counting trigram overlap: 100%|██████████| 7165/7165 [00:01<00:00, 4757.64it/s]\n","Counting named entity overlap: 100%|██████████| 7165/7165 [08:55<00:00, 13.39it/s]\n","Counting spelling errors: 100%|██████████| 7165/7165 [00:00<00:00, 7550.05it/s]\n","Tokenizing Prompts: 100%|██████████| 2/2 [00:00<00:00, 1999.19it/s]\n","Tokenizing Summaries: 100%|██████████| 4/4 [00:00<00:00, 3997.43it/s]\n","Tokenizing Summaries: 100%|██████████| 4/4 [00:00<00:00, 4003.15it/s]\n","Tokenizing Summaries: 100%|██████████| 4/4 [00:00<00:00, 4125.21it/s]\n","Counting non-stop word overlap: 100%|██████████| 4/4 [00:00<?, ?it/s]\n","Counting unigram overlap: 100%|██████████| 4/4 [00:00<00:00, 4001.24it/s]\n","Counting bigram overlap: 100%|██████████| 4/4 [00:00<00:00, 4004.11it/s]\n","Counting trigram overlap: 100%|██████████| 4/4 [00:00<?, ?it/s]\n","Counting named entity overlap: 100%|██████████| 4/4 [00:00<00:00, 137.78it/s]\n","Counting spelling errors: 100%|██████████| 4/4 [00:00<?, ?it/s]\n"]}],"source":["train = Preprocessor.run(prompts_train, summaries_train)\n","test = Preprocessor.run(prompts_test, summaries_test)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>student_id</th>\n","      <th>prompt_id</th>\n","      <th>text</th>\n","      <th>content</th>\n","      <th>wording</th>\n","      <th>summary_tokens</th>\n","      <th>prompt_question</th>\n","      <th>prompt_title</th>\n","      <th>prompt_text</th>\n","      <th>prompt_tokens</th>\n","      <th>prompt_length</th>\n","      <th>summary_length</th>\n","      <th>non_stop_word_overlap</th>\n","      <th>unigram_overlap</th>\n","      <th>bigram_overlap</th>\n","      <th>trigram_overlap</th>\n","      <th>ner_overlap</th>\n","      <th>spelling_error_count</th>\n","      <th>token_length_ratio</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000e8c3c7ddb</td>\n","      <td>814d6b</td>\n","      <td>The third wave was an experimentto see how peo...</td>\n","      <td>0.205683</td>\n","      <td>0.380538</td>\n","      <td>[▁The, ▁third, ▁wave, ▁was, ▁an, ▁experiment, ...</td>\n","      <td>Summarize how the Third Wave developed over su...</td>\n","      <td>The Third Wave</td>\n","      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n","      <td>[▁Background, ▁The, ▁Third, ▁Wave, ▁experiment...</td>\n","      <td>671</td>\n","      <td>69</td>\n","      <td>9</td>\n","      <td>26</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0.102832</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0020ae56ffbf</td>\n","      <td>ebad26</td>\n","      <td>They would rub it up with soda to make the sme...</td>\n","      <td>-0.548304</td>\n","      <td>0.506755</td>\n","      <td>[▁They, ▁would, ▁rub, ▁it, ▁up, ▁with, ▁soda, ...</td>\n","      <td>Summarize the various ways the factory would u...</td>\n","      <td>Excerpt from The Jungle</td>\n","      <td>With one member trimming beef in a cannery, an...</td>\n","      <td>[▁With, ▁one, ▁member, ▁trimming, ▁beef, ▁in, ...</td>\n","      <td>1137</td>\n","      <td>56</td>\n","      <td>14</td>\n","      <td>34</td>\n","      <td>22</td>\n","      <td>10</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.049252</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>004e978e639e</td>\n","      <td>3b9047</td>\n","      <td>In Egypt, there were many occupations and soci...</td>\n","      <td>3.128928</td>\n","      <td>4.231226</td>\n","      <td>[▁In, ▁Egypt, ,, ▁there, ▁were, ▁many, ▁occupa...</td>\n","      <td>In complete sentences, summarize the structure...</td>\n","      <td>Egyptian Social Structure</td>\n","      <td>Egyptian society was structured like a pyramid...</td>\n","      <td>[▁Egyptian, ▁society, ▁was, ▁structured, ▁like...</td>\n","      <td>651</td>\n","      <td>285</td>\n","      <td>54</td>\n","      <td>84</td>\n","      <td>56</td>\n","      <td>26</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>0.437788</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>005ab0199905</td>\n","      <td>3b9047</td>\n","      <td>The highest class was Pharaohs these people we...</td>\n","      <td>-0.210614</td>\n","      <td>-0.471415</td>\n","      <td>[▁The, ▁highest, ▁class, ▁was, ▁Pharaoh, s, ▁t...</td>\n","      <td>In complete sentences, summarize the structure...</td>\n","      <td>Egyptian Social Structure</td>\n","      <td>Egyptian society was structured like a pyramid...</td>\n","      <td>[▁Egyptian, ▁society, ▁was, ▁structured, ▁like...</td>\n","      <td>651</td>\n","      <td>43</td>\n","      <td>10</td>\n","      <td>19</td>\n","      <td>10</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0.066052</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0070c9e7af47</td>\n","      <td>814d6b</td>\n","      <td>The Third Wave developed  rapidly because the ...</td>\n","      <td>3.272894</td>\n","      <td>3.219757</td>\n","      <td>[▁The, ▁Third, ▁Wave, ▁developed, ▁rapidly, ▁b...</td>\n","      <td>Summarize how the Third Wave developed over su...</td>\n","      <td>The Third Wave</td>\n","      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n","      <td>[▁Background, ▁The, ▁Third, ▁Wave, ▁experiment...</td>\n","      <td>671</td>\n","      <td>253</td>\n","      <td>29</td>\n","      <td>58</td>\n","      <td>27</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>11</td>\n","      <td>0.377049</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7160</th>\n","      <td>ff7c7e70df07</td>\n","      <td>ebad26</td>\n","      <td>They used all sorts of chemical concoctions to...</td>\n","      <td>0.205683</td>\n","      <td>0.380538</td>\n","      <td>[▁They, ▁used, ▁all, ▁sorts, ▁of, ▁chemical, ▁...</td>\n","      <td>Summarize the various ways the factory would u...</td>\n","      <td>Excerpt from The Jungle</td>\n","      <td>With one member trimming beef in a cannery, an...</td>\n","      <td>[▁With, ▁one, ▁member, ▁trimming, ▁beef, ▁in, ...</td>\n","      <td>1137</td>\n","      <td>78</td>\n","      <td>18</td>\n","      <td>37</td>\n","      <td>40</td>\n","      <td>34</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.068602</td>\n","    </tr>\n","    <tr>\n","      <th>7161</th>\n","      <td>ffc34d056498</td>\n","      <td>3b9047</td>\n","      <td>The lowest classes are slaves and farmers slav...</td>\n","      <td>-0.308448</td>\n","      <td>0.048171</td>\n","      <td>[▁The, ▁lowest, ▁classes, ▁are, ▁slaves, ▁and,...</td>\n","      <td>In complete sentences, summarize the structure...</td>\n","      <td>Egyptian Social Structure</td>\n","      <td>Egyptian society was structured like a pyramid...</td>\n","      <td>[▁Egyptian, ▁society, ▁was, ▁structured, ▁like...</td>\n","      <td>651</td>\n","      <td>56</td>\n","      <td>14</td>\n","      <td>24</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0.086022</td>\n","    </tr>\n","    <tr>\n","      <th>7162</th>\n","      <td>ffd1576d2e1b</td>\n","      <td>3b9047</td>\n","      <td>they sorta made people start workin...</td>\n","      <td>-1.408180</td>\n","      <td>-0.493603</td>\n","      <td>[▁they, ▁sorta, ▁made, ▁people, ▁start, ▁worki...</td>\n","      <td>In complete sentences, summarize the structure...</td>\n","      <td>Egyptian Social Structure</td>\n","      <td>Egyptian society was structured like a pyramid...</td>\n","      <td>[▁Egyptian, ▁society, ▁was, ▁structured, ▁like...</td>\n","      <td>651</td>\n","      <td>66</td>\n","      <td>12</td>\n","      <td>24</td>\n","      <td>7</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>8</td>\n","      <td>0.101382</td>\n","    </tr>\n","    <tr>\n","      <th>7163</th>\n","      <td>ffe4a98093b2</td>\n","      <td>39c16e</td>\n","      <td>An ideal tragety has three elements that make ...</td>\n","      <td>-0.393310</td>\n","      <td>0.627128</td>\n","      <td>[▁An, ▁ideal, ▁trag, ety, ▁has, ▁three, ▁eleme...</td>\n","      <td>Summarize at least 3 elements of an ideal trag...</td>\n","      <td>On Tragedy</td>\n","      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n","      <td>[▁Chapter, ▁13, ▁As, ▁the, ▁sequel, ▁to, ▁what...</td>\n","      <td>721</td>\n","      <td>66</td>\n","      <td>10</td>\n","      <td>24</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0.091540</td>\n","    </tr>\n","    <tr>\n","      <th>7164</th>\n","      <td>fffbccfd8a08</td>\n","      <td>ebad26</td>\n","      <td>The meat would smell sour but the would \"rub i...</td>\n","      <td>1.771596</td>\n","      <td>0.547742</td>\n","      <td>[▁The, ▁meat, ▁would, ▁smell, ▁sour, ▁but, ▁th...</td>\n","      <td>Summarize the various ways the factory would u...</td>\n","      <td>Excerpt from The Jungle</td>\n","      <td>With one member trimming beef in a cannery, an...</td>\n","      <td>[▁With, ▁one, ▁member, ▁trimming, ▁beef, ▁in, ...</td>\n","      <td>1137</td>\n","      <td>118</td>\n","      <td>27</td>\n","      <td>55</td>\n","      <td>30</td>\n","      <td>11</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0.103782</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7165 rows × 19 columns</p>\n","</div>"],"text/plain":["        student_id prompt_id   \n","0     000e8c3c7ddb    814d6b  \\\n","1     0020ae56ffbf    ebad26   \n","2     004e978e639e    3b9047   \n","3     005ab0199905    3b9047   \n","4     0070c9e7af47    814d6b   \n","...            ...       ...   \n","7160  ff7c7e70df07    ebad26   \n","7161  ffc34d056498    3b9047   \n","7162  ffd1576d2e1b    3b9047   \n","7163  ffe4a98093b2    39c16e   \n","7164  fffbccfd8a08    ebad26   \n","\n","                                                   text   content   wording   \n","0     The third wave was an experimentto see how peo...  0.205683  0.380538  \\\n","1     They would rub it up with soda to make the sme... -0.548304  0.506755   \n","2     In Egypt, there were many occupations and soci...  3.128928  4.231226   \n","3     The highest class was Pharaohs these people we... -0.210614 -0.471415   \n","4     The Third Wave developed  rapidly because the ...  3.272894  3.219757   \n","...                                                 ...       ...       ...   \n","7160  They used all sorts of chemical concoctions to...  0.205683  0.380538   \n","7161  The lowest classes are slaves and farmers slav... -0.308448  0.048171   \n","7162             they sorta made people start workin... -1.408180 -0.493603   \n","7163  An ideal tragety has three elements that make ... -0.393310  0.627128   \n","7164  The meat would smell sour but the would \"rub i...  1.771596  0.547742   \n","\n","                                         summary_tokens   \n","0     [▁The, ▁third, ▁wave, ▁was, ▁an, ▁experiment, ...  \\\n","1     [▁They, ▁would, ▁rub, ▁it, ▁up, ▁with, ▁soda, ...   \n","2     [▁In, ▁Egypt, ,, ▁there, ▁were, ▁many, ▁occupa...   \n","3     [▁The, ▁highest, ▁class, ▁was, ▁Pharaoh, s, ▁t...   \n","4     [▁The, ▁Third, ▁Wave, ▁developed, ▁rapidly, ▁b...   \n","...                                                 ...   \n","7160  [▁They, ▁used, ▁all, ▁sorts, ▁of, ▁chemical, ▁...   \n","7161  [▁The, ▁lowest, ▁classes, ▁are, ▁slaves, ▁and,...   \n","7162  [▁they, ▁sorta, ▁made, ▁people, ▁start, ▁worki...   \n","7163  [▁An, ▁ideal, ▁trag, ety, ▁has, ▁three, ▁eleme...   \n","7164  [▁The, ▁meat, ▁would, ▁smell, ▁sour, ▁but, ▁th...   \n","\n","                                        prompt_question   \n","0     Summarize how the Third Wave developed over su...  \\\n","1     Summarize the various ways the factory would u...   \n","2     In complete sentences, summarize the structure...   \n","3     In complete sentences, summarize the structure...   \n","4     Summarize how the Third Wave developed over su...   \n","...                                                 ...   \n","7160  Summarize the various ways the factory would u...   \n","7161  In complete sentences, summarize the structure...   \n","7162  In complete sentences, summarize the structure...   \n","7163  Summarize at least 3 elements of an ideal trag...   \n","7164  Summarize the various ways the factory would u...   \n","\n","                   prompt_title   \n","0                The Third Wave  \\\n","1       Excerpt from The Jungle   \n","2     Egyptian Social Structure   \n","3     Egyptian Social Structure   \n","4                The Third Wave   \n","...                         ...   \n","7160    Excerpt from The Jungle   \n","7161  Egyptian Social Structure   \n","7162  Egyptian Social Structure   \n","7163                 On Tragedy   \n","7164    Excerpt from The Jungle   \n","\n","                                            prompt_text   \n","0     Background \\r\\nThe Third Wave experiment took ...  \\\n","1     With one member trimming beef in a cannery, an...   \n","2     Egyptian society was structured like a pyramid...   \n","3     Egyptian society was structured like a pyramid...   \n","4     Background \\r\\nThe Third Wave experiment took ...   \n","...                                                 ...   \n","7160  With one member trimming beef in a cannery, an...   \n","7161  Egyptian society was structured like a pyramid...   \n","7162  Egyptian society was structured like a pyramid...   \n","7163  Chapter 13 \\r\\nAs the sequel to what has alrea...   \n","7164  With one member trimming beef in a cannery, an...   \n","\n","                                          prompt_tokens  prompt_length   \n","0     [▁Background, ▁The, ▁Third, ▁Wave, ▁experiment...            671  \\\n","1     [▁With, ▁one, ▁member, ▁trimming, ▁beef, ▁in, ...           1137   \n","2     [▁Egyptian, ▁society, ▁was, ▁structured, ▁like...            651   \n","3     [▁Egyptian, ▁society, ▁was, ▁structured, ▁like...            651   \n","4     [▁Background, ▁The, ▁Third, ▁Wave, ▁experiment...            671   \n","...                                                 ...            ...   \n","7160  [▁With, ▁one, ▁member, ▁trimming, ▁beef, ▁in, ...           1137   \n","7161  [▁Egyptian, ▁society, ▁was, ▁structured, ▁like...            651   \n","7162  [▁Egyptian, ▁society, ▁was, ▁structured, ▁like...            651   \n","7163  [▁Chapter, ▁13, ▁As, ▁the, ▁sequel, ▁to, ▁what...            721   \n","7164  [▁With, ▁one, ▁member, ▁trimming, ▁beef, ▁in, ...           1137   \n","\n","      summary_length  non_stop_word_overlap  unigram_overlap  bigram_overlap   \n","0                 69                      9               26               5  \\\n","1                 56                     14               34              22   \n","2                285                     54               84              56   \n","3                 43                     10               19              10   \n","4                253                     29               58              27   \n","...              ...                    ...              ...             ...   \n","7160              78                     18               37              40   \n","7161              56                     14               24               6   \n","7162              66                     12               24               7   \n","7163              66                     10               24               4   \n","7164             118                     27               55              30   \n","\n","      trigram_overlap  ner_overlap  spelling_error_count  token_length_ratio  \n","0                   0            2                     2            0.102832  \n","1                  10            0                     0            0.049252  \n","2                  26            5                     3            0.437788  \n","3                   6            0                     4            0.066052  \n","4                   5            3                    11            0.377049  \n","...               ...          ...                   ...                 ...  \n","7160               34            0                     0            0.068602  \n","7161                1            0                     2            0.086022  \n","7162                1            1                     8            0.101382  \n","7163                0            0                     1            0.091540  \n","7164               11            0                     1            0.103782  \n","\n","[7165 rows x 19 columns]"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["train"]},{"cell_type":"markdown","metadata":{},"source":["### Metrics functions"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    rmse = mean_squared_error(labels, predictions, squared=False)\n","    return {\"rmse\": rmse}\n","\n","\n","def compute_mcrmse(eval_pred):\n","    \"\"\"\n","    Calculates mean columnwise root mean squared error\n","    https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/overview/evaluation\n","    \"\"\"\n","    preds, labels = eval_pred\n","\n","    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))\n","    mcrmse = np.mean(col_rmse)\n","\n","    return {\n","        \"content_rmse\": col_rmse[0],\n","        \"wording_rmse\": col_rmse[1],\n","        \"mcrmse\": mcrmse,\n","    }\n","\n","\n","def compt_score(content_true, content_pred, wording_true, wording_pred):\n","    content_score = mean_squared_error(content_true, content_pred) ** (1 / 2)\n","    wording_score = mean_squared_error(wording_true, wording_pred) ** (1 / 2)\n","\n","    return (content_score + wording_score) / 2"]},{"cell_type":"markdown","metadata":{},"source":["## Train LLM"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["class SummaryRegressor:\n","    def __init__(\n","        self,\n","        model_name: str,\n","        model_dir: str,\n","        target: list[str],\n","        hidden_dropout_prob: float,\n","        attention_probs_dropout_prob: float,\n","        max_length: int,\n","    ):\n","        self.inputs = [\"prompt_text\", \"prompt_title\", \"prompt_question\", \"text\"]\n","        self.input_col = \"input\"\n","\n","        self.target = target\n","        self.target_cols = [target]\n","\n","        self.model_name = model_name\n","        self.model_dir = model_dir\n","        self.max_length = max_length\n","\n","        # self.tokenizer = AutoTokenizer.from_pretrained(f\"/kaggle/input/{model_name}\")\n","        # self.model_config = AutoConfig.from_pretrained(f\"/kaggle/input/{model_name}\")\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        self.model_config = AutoConfig.from_pretrained(model_name)\n","        self.model_config.update(\n","            {\n","                \"hidden_dropout_prob\": CFG.hidden_dropout_prob,\n","                \"attention_probs_dropout_prob\": CFG.attention_probs_dropout_prob,\n","                \"num_labels\": 2,\n","                \"problem_type\": \"regression\",\n","            }\n","        )\n","        self.data_collator = DataCollatorWithPadding(self.tokenizer)\n","\n","        seed_everything(seed=42)\n","\n","    def tokenize_function(self, examples: pd.DataFrame):\n","        labels = [examples[self.target]]\n","        tokenized = self.tokenizer(\n","            examples[self.input_col],\n","            padding=False,\n","            truncation=True,\n","            max_length=self.max_length,\n","        )\n","        return {\n","            **tokenized,\n","            \"labels\": labels,\n","        }\n","\n","    def tokenize_function_test(self, examples: pd.DataFrame):\n","        tokenized = self.tokenizer(\n","            examples[self.input_col],\n","            padding=False,\n","            truncation=True,\n","            max_length=self.max_length,\n","        )\n","        return tokenized\n","\n","    def train(\n","        self,\n","        fold: int,\n","        train_df: pd.DataFrame,\n","        val_df: pd.DataFrame,\n","        batch_size: int,\n","        learning_rate: float,\n","        weight_decay: float,\n","        num_train_epochs: float,\n","        save_steps: int,\n","    ) -> None:\n","        sep = self.tokenizer.sep_token\n","        train_df[self.input_col] = (\n","            train_df[\"prompt_title\"]\n","            + sep\n","            + train_df[\"prompt_question\"]\n","            + sep\n","            + train_df[\"text\"]\n","        )\n","        val_df[self.input_col] = (\n","            val_df[\"prompt_title\"]\n","            + sep\n","            + val_df[\"prompt_question\"]\n","            + sep\n","            + val_df[\"text\"]\n","        )\n","\n","        train_df = train_df[[self.input_col] + self.target_cols]\n","        val_df = val_df[[self.input_col] + self.target_cols]\n","\n","        train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n","        val_dataset = Dataset.from_pandas(val_df, preserve_index=False)\n","\n","        train_dataset_tokenized = train_dataset.map(\n","            self.tokenize_function, batched=False\n","        )\n","        val_dataset_tokenized = val_dataset.map(self.tokenize_function, batched=False)\n","\n","        model_content = AutoModelForSequenceClassification.from_pretrained(\n","            f\"/kaggle/input/{self.model_name}\",\n","            config=self.model_config,\n","        )\n","\n","        model_fold_dir = f\"{self.model_dir}/fold_{fold}\"\n","\n","        training_args = TrainingArguments(\n","            output_dir=model_fold_dir,\n","            num_train_epochs=num_train_epochs,\n","            per_device_train_batch_size=batch_size,\n","            per_device_eval_batch_size=batch_size,\n","            learning_rate=learning_rate,\n","            weight_decay=weight_decay,\n","            save_strategy=\"steps\",\n","            save_steps=save_steps,\n","            save_total_limit=1,\n","            load_best_model_at_end=True,\n","            metric_for_best_model=\"rmse\",\n","            greater_is_better=False,\n","            evaluation_strategy=\"steps\",\n","            eval_steps=save_steps,\n","            # disable_tqdm=True,\n","            report_to=\"none\",\n","        )\n","\n","        trainer = Trainer(\n","            model=model_content,\n","            args=training_args,\n","            train_dataset=train_dataset_tokenized,\n","            eval_dataset=val_dataset_tokenized,\n","            compute_metrics=compute_metrics,\n","            data_collator=self.data_collator,\n","        )\n","\n","        trainer.train()\n","\n","        model_content.save_pretrained(self.model_dir)\n","        self.tokenizer.save_pretrained(self.model_dir)\n","\n","    def predict(\n","        self,\n","        test_df: pd.DataFrame,\n","        fold: int,\n","    ) -> np.array:\n","        sep = self.tokenizer.sep_token\n","        test_df[self.input_col] = (\n","            test_df[\"prompt_title\"]\n","            + sep\n","            + test_df[\"prompt_question\"]\n","            + sep\n","            + test_df[\"text\"]\n","        )\n","        test_df = test_df[[self.input_col]]\n","        test_dataset = Dataset.from_pandas(test_df, preserve_index=False)\n","        test_dataset_tokenized = test_dataset.map(\n","            self.tokenize_function_test, batched=False\n","        )\n","        model_fold_dir = f\"{self.model_dir}/fold_{fold}\"\n","\n","        model_content = AutoModelForSequenceClassification.from_pretrained(\n","            self.model_dir\n","        )\n","        model_content.eval()\n","\n","        test_args = TrainingArguments(\n","            output_dir=model_fold_dir,\n","            do_train=False,\n","            do_predict=True,\n","            per_device_eval_batch_size=4,\n","            dataloader_drop_last=False,\n","            # disable_tqdm=True,\n","        )\n","\n","        infer_trainer = Trainer(\n","            model=model_content,\n","            args=test_args,\n","            data_collator=self.data_collator,\n","        )\n","\n","        predictions = infer_trainer.predict(test_dataset_tokenized)\n","\n","        return predictions"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>student_id</th>\n","      <th>prompt_id</th>\n","      <th>text</th>\n","      <th>content</th>\n","      <th>wording</th>\n","      <th>summary_tokens</th>\n","      <th>prompt_question</th>\n","      <th>prompt_title</th>\n","      <th>prompt_text</th>\n","      <th>prompt_tokens</th>\n","      <th>prompt_length</th>\n","      <th>summary_length</th>\n","      <th>non_stop_word_overlap</th>\n","      <th>unigram_overlap</th>\n","      <th>bigram_overlap</th>\n","      <th>trigram_overlap</th>\n","      <th>ner_overlap</th>\n","      <th>spelling_error_count</th>\n","      <th>token_length_ratio</th>\n","      <th>fold</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000e8c3c7ddb</td>\n","      <td>814d6b</td>\n","      <td>The third wave was an experimentto see how peo...</td>\n","      <td>0.205683</td>\n","      <td>0.380538</td>\n","      <td>[▁The, ▁third, ▁wave, ▁was, ▁an, ▁experiment, ...</td>\n","      <td>Summarize how the Third Wave developed over su...</td>\n","      <td>The Third Wave</td>\n","      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n","      <td>[▁Background, ▁The, ▁Third, ▁Wave, ▁experiment...</td>\n","      <td>671</td>\n","      <td>69</td>\n","      <td>9</td>\n","      <td>26</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0.102832</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0020ae56ffbf</td>\n","      <td>ebad26</td>\n","      <td>They would rub it up with soda to make the sme...</td>\n","      <td>-0.548304</td>\n","      <td>0.506755</td>\n","      <td>[▁They, ▁would, ▁rub, ▁it, ▁up, ▁with, ▁soda, ...</td>\n","      <td>Summarize the various ways the factory would u...</td>\n","      <td>Excerpt from The Jungle</td>\n","      <td>With one member trimming beef in a cannery, an...</td>\n","      <td>[▁With, ▁one, ▁member, ▁trimming, ▁beef, ▁in, ...</td>\n","      <td>1137</td>\n","      <td>56</td>\n","      <td>14</td>\n","      <td>34</td>\n","      <td>22</td>\n","      <td>10</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.049252</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>004e978e639e</td>\n","      <td>3b9047</td>\n","      <td>In Egypt, there were many occupations and soci...</td>\n","      <td>3.128928</td>\n","      <td>4.231226</td>\n","      <td>[▁In, ▁Egypt, ,, ▁there, ▁were, ▁many, ▁occupa...</td>\n","      <td>In complete sentences, summarize the structure...</td>\n","      <td>Egyptian Social Structure</td>\n","      <td>Egyptian society was structured like a pyramid...</td>\n","      <td>[▁Egyptian, ▁society, ▁was, ▁structured, ▁like...</td>\n","      <td>651</td>\n","      <td>285</td>\n","      <td>54</td>\n","      <td>84</td>\n","      <td>56</td>\n","      <td>26</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>0.437788</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>005ab0199905</td>\n","      <td>3b9047</td>\n","      <td>The highest class was Pharaohs these people we...</td>\n","      <td>-0.210614</td>\n","      <td>-0.471415</td>\n","      <td>[▁The, ▁highest, ▁class, ▁was, ▁Pharaoh, s, ▁t...</td>\n","      <td>In complete sentences, summarize the structure...</td>\n","      <td>Egyptian Social Structure</td>\n","      <td>Egyptian society was structured like a pyramid...</td>\n","      <td>[▁Egyptian, ▁society, ▁was, ▁structured, ▁like...</td>\n","      <td>651</td>\n","      <td>43</td>\n","      <td>10</td>\n","      <td>19</td>\n","      <td>10</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0.066052</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0070c9e7af47</td>\n","      <td>814d6b</td>\n","      <td>The Third Wave developed  rapidly because the ...</td>\n","      <td>3.272894</td>\n","      <td>3.219757</td>\n","      <td>[▁The, ▁Third, ▁Wave, ▁developed, ▁rapidly, ▁b...</td>\n","      <td>Summarize how the Third Wave developed over su...</td>\n","      <td>The Third Wave</td>\n","      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n","      <td>[▁Background, ▁The, ▁Third, ▁Wave, ▁experiment...</td>\n","      <td>671</td>\n","      <td>253</td>\n","      <td>29</td>\n","      <td>58</td>\n","      <td>27</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>11</td>\n","      <td>0.377049</td>\n","      <td>3.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     student_id prompt_id                                               text   \n","0  000e8c3c7ddb    814d6b  The third wave was an experimentto see how peo...  \\\n","1  0020ae56ffbf    ebad26  They would rub it up with soda to make the sme...   \n","2  004e978e639e    3b9047  In Egypt, there were many occupations and soci...   \n","3  005ab0199905    3b9047  The highest class was Pharaohs these people we...   \n","4  0070c9e7af47    814d6b  The Third Wave developed  rapidly because the ...   \n","\n","    content   wording                                     summary_tokens   \n","0  0.205683  0.380538  [▁The, ▁third, ▁wave, ▁was, ▁an, ▁experiment, ...  \\\n","1 -0.548304  0.506755  [▁They, ▁would, ▁rub, ▁it, ▁up, ▁with, ▁soda, ...   \n","2  3.128928  4.231226  [▁In, ▁Egypt, ,, ▁there, ▁were, ▁many, ▁occupa...   \n","3 -0.210614 -0.471415  [▁The, ▁highest, ▁class, ▁was, ▁Pharaoh, s, ▁t...   \n","4  3.272894  3.219757  [▁The, ▁Third, ▁Wave, ▁developed, ▁rapidly, ▁b...   \n","\n","                                     prompt_question   \n","0  Summarize how the Third Wave developed over su...  \\\n","1  Summarize the various ways the factory would u...   \n","2  In complete sentences, summarize the structure...   \n","3  In complete sentences, summarize the structure...   \n","4  Summarize how the Third Wave developed over su...   \n","\n","                prompt_title   \n","0             The Third Wave  \\\n","1    Excerpt from The Jungle   \n","2  Egyptian Social Structure   \n","3  Egyptian Social Structure   \n","4             The Third Wave   \n","\n","                                         prompt_text   \n","0  Background \\r\\nThe Third Wave experiment took ...  \\\n","1  With one member trimming beef in a cannery, an...   \n","2  Egyptian society was structured like a pyramid...   \n","3  Egyptian society was structured like a pyramid...   \n","4  Background \\r\\nThe Third Wave experiment took ...   \n","\n","                                       prompt_tokens  prompt_length   \n","0  [▁Background, ▁The, ▁Third, ▁Wave, ▁experiment...            671  \\\n","1  [▁With, ▁one, ▁member, ▁trimming, ▁beef, ▁in, ...           1137   \n","2  [▁Egyptian, ▁society, ▁was, ▁structured, ▁like...            651   \n","3  [▁Egyptian, ▁society, ▁was, ▁structured, ▁like...            651   \n","4  [▁Background, ▁The, ▁Third, ▁Wave, ▁experiment...            671   \n","\n","   summary_length  non_stop_word_overlap  unigram_overlap  bigram_overlap   \n","0              69                      9               26               5  \\\n","1              56                     14               34              22   \n","2             285                     54               84              56   \n","3              43                     10               19              10   \n","4             253                     29               58              27   \n","\n","   trigram_overlap  ner_overlap  spelling_error_count  token_length_ratio   \n","0                0            2                     2            0.102832  \\\n","1               10            0                     0            0.049252   \n","2               26            5                     3            0.437788   \n","3                6            0                     4            0.066052   \n","4                5            3                    11            0.377049   \n","\n","   fold  \n","0   3.0  \n","1   2.0  \n","2   1.0  \n","3   1.0  \n","4   3.0  "]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["# Create folds\n","\n","gkf = GroupKFold(n_splits=CFG.n_splits)\n","\n","for i, (_, val_index) in enumerate(gkf.split(train, groups=train[\"prompt_id\"])):\n","    train.loc[val_index, \"fold\"] = i\n","\n","train.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Cross Validation"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["def train_folds(\n","    train_df: pd.DataFrame,\n","    model_name: str,\n","    target: list[str],\n","    hidden_dropout_prob: float,\n","    attention_probs_dropout_prob: float,\n","    max_length: int,\n","    batch_size: int,\n","    learning_rate: float,\n","    weight_decay: float,\n","    num_train_epochs: float,\n","    save_steps: int,\n","    n_splits: int,\n","    save_each_model: bool,\n","):\n","    if os.path.exists(f\"{model_name}\"):\n","        shutil.rmtree(f\"{model_name}\")\n","    os.mkdir(f\"{model_name}\")\n","\n","    for fold in range(n_splits):\n","        print(f\"Training fold {fold}\")\n","\n","        train_data = train_df[train_df[\"fold\"] != fold]\n","        val_data = train_df[train_df[\"fold\"] == fold]\n","\n","        if save_each_model:\n","            model_dir = f\"{model_name}/fold_{fold}\"\n","        else:\n","            model_dir = model_name\n","\n","        model = SummaryRegressor(\n","            model_name=model_name,\n","            model_dir=model_dir,\n","            target=target,\n","            hidden_dropout_prob=hidden_dropout_prob,\n","            attention_probs_dropout_prob=attention_probs_dropout_prob,\n","            max_length=max_length,\n","        )\n","\n","        model.train(\n","            fold=fold,\n","            train_df=train_data,\n","            val_df=val_data,\n","            batch_size=batch_size,\n","            learning_rate=learning_rate,\n","            weight_decay=weight_decay,\n","            num_train_epochs=num_train_epochs,\n","            save_steps=save_steps,\n","        )\n","\n","\n","def validate(\n","    train_df: pd.DataFrame,\n","    target: list[str],\n","    save_each_model: bool,\n","    model_name: str,\n","    hidden_dropout_prob: float,\n","    attention_probs_dropout_prob: float,\n","    max_length: int,\n",") -> pd.DataFrame:\n","    for fold in range(CFG.n_splits):\n","        if save_each_model:\n","            model_dir = f\"{model_name}/fold_{fold}\"\n","        else:\n","            model_dir = model_name\n","        val_data = train_df[train_df[\"fold\"] == fold]\n","        model = SummaryRegressor(\n","            model_name=model_name,\n","            model_dir=model_dir,\n","            target=target,\n","            hidden_dropout_prob=hidden_dropout_prob,\n","            attention_probs_dropout_prob=attention_probs_dropout_prob,\n","            max_length=max_length,\n","        )\n","\n","        predictions = model.predict(test_df=val_data, fold=fold)\n","\n","        train_df.loc[val_data.index, \"content_pred\"] = predictions.predictions[:, 0]\n","        train_df.loc[val_data.index, \"wording_pred\"] = predictions.predictions[:, 1]\n","\n","    return train_df\n","\n","\n","def predict(\n","    test_df: pd.DataFrame,\n","    target: list[str],\n","    save_each_model: bool,\n","    model_name: str,\n","    hidden_dropout_prob: float,\n","    attention_probs_dropout_prob: float,\n","    max_length: int,\n",") -> pd.DataFrame:\n","    for fold in range(CFG.n_splits):\n","        if save_each_model:\n","            model_dir = f\"{model_name}/fold_{fold}\"\n","        else:\n","            model_dir = model_name\n","        model = SummaryRegressor(\n","            model_name=model_name,\n","            model_dir=model_dir,\n","            target=target,\n","            hidden_dropout_prob=hidden_dropout_prob,\n","            attention_probs_dropout_prob=attention_probs_dropout_prob,\n","            max_length=max_length,\n","        )\n","\n","        predictions = model.predict(test_df=test_df, fold=fold)\n","\n","        test_df.loc[:, \"content_pred\"] = predictions.predictions[:, 0]\n","        test_df.loc[:, \"wording_pred\"] = predictions.predictions[:, 1]\n","\n","    return test_df"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"ename":"FileNotFoundError","evalue":"[WinError 3] The system cannot find the path specified: 'MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m target \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mwording\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m train_folds(\n\u001b[0;32m      3\u001b[0m     train_df\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m      4\u001b[0m     model_name\u001b[39m=\u001b[39;49mCFG\u001b[39m.\u001b[39;49mmodel_name,\n\u001b[0;32m      5\u001b[0m     target\u001b[39m=\u001b[39;49mtarget,\n\u001b[0;32m      6\u001b[0m     hidden_dropout_prob\u001b[39m=\u001b[39;49mCFG\u001b[39m.\u001b[39;49mhidden_dropout_prob,\n\u001b[0;32m      7\u001b[0m     attention_probs_dropout_prob\u001b[39m=\u001b[39;49mCFG\u001b[39m.\u001b[39;49mattention_probs_dropout_prob,\n\u001b[0;32m      8\u001b[0m     max_length\u001b[39m=\u001b[39;49mCFG\u001b[39m.\u001b[39;49mmax_length,\n\u001b[0;32m      9\u001b[0m     batch_size\u001b[39m=\u001b[39;49mCFG\u001b[39m.\u001b[39;49mbatch_size,\n\u001b[0;32m     10\u001b[0m     learning_rate\u001b[39m=\u001b[39;49mCFG\u001b[39m.\u001b[39;49mlearning_rate,\n\u001b[0;32m     11\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mCFG\u001b[39m.\u001b[39;49mweight_decay,\n\u001b[0;32m     12\u001b[0m     num_train_epochs\u001b[39m=\u001b[39;49mCFG\u001b[39m.\u001b[39;49mnum_train_epochs,\n\u001b[0;32m     13\u001b[0m     save_steps\u001b[39m=\u001b[39;49mCFG\u001b[39m.\u001b[39;49msave_steps,\n\u001b[0;32m     14\u001b[0m     n_splits\u001b[39m=\u001b[39;49mCFG\u001b[39m.\u001b[39;49mn_splits,\n\u001b[0;32m     15\u001b[0m     save_each_model\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     18\u001b[0m train \u001b[39m=\u001b[39m validate(\n\u001b[0;32m     19\u001b[0m     train_df\u001b[39m=\u001b[39mtrain,\n\u001b[0;32m     20\u001b[0m     target\u001b[39m=\u001b[39mtarget,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m     max_length\u001b[39m=\u001b[39mCFG\u001b[39m.\u001b[39mmax_length,\n\u001b[0;32m     26\u001b[0m )\n\u001b[0;32m     28\u001b[0m \u001b[39m# print the score\u001b[39;00m\n","Cell \u001b[1;32mIn[20], line 18\u001b[0m, in \u001b[0;36mtrain_folds\u001b[1;34m(train_df, model_name, target, hidden_dropout_prob, attention_probs_dropout_prob, max_length, batch_size, learning_rate, weight_decay, num_train_epochs, save_steps, n_splits, save_each_model)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     17\u001b[0m     shutil\u001b[39m.\u001b[39mrmtree(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m os\u001b[39m.\u001b[39;49mmkdir(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mmodel_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     20\u001b[0m \u001b[39mfor\u001b[39;00m fold \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_splits):\n\u001b[0;32m     21\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining fold \u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n","\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli'"]}],"source":["target = [\"content\", \"wording\"]\n","train_folds(\n","    train_df=train,\n","    model_name=CFG.model_name,\n","    target=target,\n","    hidden_dropout_prob=CFG.hidden_dropout_prob,\n","    attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n","    max_length=CFG.max_length,\n","    batch_size=CFG.batch_size,\n","    learning_rate=CFG.learning_rate,\n","    weight_decay=CFG.weight_decay,\n","    num_train_epochs=CFG.num_train_epochs,\n","    save_steps=CFG.save_steps,\n","    n_splits=CFG.n_splits,\n","    save_each_model=False,\n",")\n","\n","train = validate(\n","    train_df=train,\n","    target=target,\n","    save_each_model=False,\n","    model_name=CFG.model_name,\n","    hidden_dropout_prob=CFG.hidden_dropout_prob,\n","    attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n","    max_length=CFG.max_length,\n",")\n","\n","# print the score\n","print(\n","    f\"Score: {compt_score(train.content, train.content_pred, train.wording, train.wording_pred)}\"\n",")\n","\n","test = predict(\n","    test_df=test,\n","    target=target,\n","    save_each_model=False,\n","    model_name=CFG.model_name,\n","    hidden_dropout_prob=CFG.hidden_dropout_prob,\n","    attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n","    max_length=CFG.max_length,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train.head()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":4}
